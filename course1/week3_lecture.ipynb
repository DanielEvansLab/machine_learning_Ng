{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 3 lecture notes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks overview. \n",
    "A node consists of a linear model taking multiple inputs, plus there is an activation function on the output of the node.\n",
    "\n",
    "Superscript square bracket 1 refers to first layer.\n",
    "\n",
    "Superscript round bracket refers to the observation, or person, or training example.\n",
    "\n",
    "Subscript refers to the node within a layer.\n",
    "\n",
    "$$a^{[1](2)}_3 $$ refers to the value of the activation function at layer 1, node 3, person 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "Layer 2 takes output of layer 1, performs second round of regression.\n",
    "\n",
    "Input layer = x's.\n",
    "Hidden layer = nodes.\n",
    "Output layer = just one node.\n",
    "\n",
    "Input layer is $a^{[0]} $ and equals x\n",
    "\n",
    "Hidden layer is $a^{[1]}$\n",
    "\n",
    "Output layer is $a^{[2]}$\n",
    "\n",
    "2-layered network, because you don't count the input layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden layer and output layer has parameters W and b associated with it. \n",
    "\n",
    "If there are 4 nodes at layer 1, and there are 3 X's that are inputs into layer 1, then the $W^{[1]}$ parameters would stored in a (4,3) matrix. 4 rows for the 4 nodes. 3 columns for the 3 X's. \n",
    "\n",
    "For this same configuation, there would be a (4,1) $b^{[1]}$ matrix. 4 rows for the 4 nodes, 1 column for the 1 b for each model. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing neural network output.\n",
    "\n",
    "In a 2 layer model, at layer 1 node 1, you have the following computations:\n",
    "\n",
    "$z^{[1]}_1 = w^{[1]T}_1 * x + b^{[1]}_1$\n",
    "\n",
    "$a^{[1]}_1 = \\sigma(z^{[1]}_1) $\n",
    "\n",
    "You have these 2 equations for each node. With 4 nodes, for example, you have 4 z equations and 4 a equations. Just change the subscript for the different nodes in a layer.\n",
    "\n",
    "Calculating the 4 equations across the nodes in a layer using a for loop would be slow. Need to vectorize across nodes.\n",
    "\n",
    "Create a (4,3) matrix storing the W vectors for the 4 nodes in a layer. This is referred to as $W^{[1]}$\n",
    "\n",
    "Create a vector (4,1) matrix storing the b values. Referred to as $b^{[1]}$\n",
    "\n",
    "$Z^{[1]}$ is calculated for each node.\n",
    "\n",
    "\n",
    "If you have 4 nodes in layer 1 and 1 node in layer 2, the W and b matrices at layer 2 would be:\n",
    "\n",
    "$W^{[2]} = $ (1,4) matrix. 1 node, 4 inputs.\n",
    "$b^{[2]} = $ (1,1) matrix. 1 node, 1 intercept. \n",
    "\n",
    "The matrix multiplication of the weight matrix and the X matrix is quick. However, this is for just a single training example. Need to do this across multiple training examples. Next..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize across training examples\n",
    "\n",
    "$a^{[2](i)} = $ output of activation function at layer 2 for person i.\n",
    "\n",
    "For loop would loop over m training examples.\n",
    "\n",
    "for i = 1 to m:\n",
    "\n",
    "$Z^{[1](i)} = W^{T[1]}*x^{(i)} + b^{[1]}$\n",
    "    \n",
    "$a^{[1](i)} = \\sigma(Z^{[1](i)})$\n",
    "\n",
    "Don't do for loop. Vectorize.\n",
    "\n",
    "Create X matrix with $N_x$ rows and m columns. With this set-up, it's super easy to vectorize. Can just multiply W matrix by X matrix. Broadcast b addition. This outputs a matrix Z for each layer, with number of rows = number of nodes. Then apply sigmoid function to the Z vector. Do that for the two layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of activation functions\n",
    "\n",
    "Needed during back propogation. \n",
    "\n",
    "sigmoid function. \n",
    "$g(z) = 1/(1 + e^{-z})$\n",
    "\n",
    "What's the derivative of g(z)?\n",
    "\n",
    "$d/dz g(z) = g(z)*(1-g(z))$\n",
    "\n",
    "At extreme values of z, the derivative is nearly zero.\n",
    "\n",
    "tanh activation function.\n",
    "\n",
    "$d/dz g(z) = 1 - (tanh(z))^2$\n",
    "\n",
    "ReLU activation function.\n",
    "\n",
    "$d/dz g(z) = 0 if z<0; 1 if z >= 0 $\n",
    "\n",
    "Leaky ReLU\n",
    "\n",
    "$d/dz g(z) = 0.01 if z<0; 1 if z >= 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent for neural networks\n",
    "\n",
    "Parameters = $W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}$\n",
    "\n",
    "$n_x = n^{[0]}$ input features or variables\n",
    "\n",
    "$n^{[1]}$ hidden units\n",
    "\n",
    "$n^{[2]}$ = output layer containing one node\n",
    "\n",
    "$W^{[1]} = (n^{[1]}, n^{[0]})$ matrix\n",
    "\n",
    "$b^{[1]} = (n^{[1]}, 1)$ matrix\n",
    "\n",
    "$W^{[2]} = (n^{[2]}, n^{[1]})$\n",
    "\n",
    "$b^{[2]} = (n^{[2]}, 1)$\n",
    "\n",
    "Cost fxn = average loss function across m samples\n",
    "\n",
    "Gradient descent.\n",
    "Repeat:\n",
    "    compute predicted probabilities\n",
    "    compute dw[1], db[1], dw[2], db2\n",
    "    update parameters\n",
    "    w[1] = w[1] - alpha dw[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "b1 = np.zeros(2)\n",
    "print(b1)\n",
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
